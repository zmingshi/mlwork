{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# etc/bin/python\n",
    "# -*- encoding: utf-8 -*-\n",
    "\n",
    "from time import time\n",
    "from gensim.models import Word2Vec\n",
    "import sys\n",
    "import os\n",
    "\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class LoadCorpora(object):\n",
    "    def __init__(self, s):\n",
    "        self.path = s\n",
    "\n",
    "    def __iter__(self):\n",
    "        f = open(self.path, 'r')\n",
    "        for line in f:\n",
    "            yield line.split(' ')\n",
    "\n",
    "\n",
    "def print_list(a):\n",
    "    for i, s in enumerate(a):\n",
    "        if i != 0:\n",
    "            print '+',\n",
    "        print s,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('news.model'):\n",
    "    sentences = LoadCorpora('../dataset/news.dat')\n",
    "    t_start = time()\n",
    "    model = Word2Vec(sentences, size=200, min_count=5, workers=8)  # 词向量维度为200，丢弃出现次数少于5次的词\n",
    "    model.save('news.model')\n",
    "    print 'OK:', time() - t_start\n",
    "\n",
    "model = Word2Vec.load('news.model')\n",
    "print '词典中词的个数：', len(model.raw_vocab)\n",
    "for i, word in enumerate(model.raw_vocab):\n",
    "    print word,\n",
    "    if i % 25 == 24:\n",
    "        print\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "intrested_words = ('中国', '手机', '学习', '人民', '名义')\n",
    "print '特征向量：'\n",
    "for word in intrested_words:\n",
    "    print word, len(model[word]), model[word]\n",
    "for word in intrested_words:\n",
    "    result = model.most_similar(word)\n",
    "    print '与', word, '最相近的词：'\n",
    "    for w, s in result:\n",
    "        print '\\t', w, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "words = ('中国', '祖国', '毛泽东', '人民')\n",
    "for i in range(len(words)):\n",
    "    w1 = words[i]\n",
    "    for j in range(i + 1, len(words)):\n",
    "        w2 = words[j]\n",
    "        print '%s 和 %s 的相似度为：%.6f' % (w1, w2, model.similarity(w1, w2))\n",
    "\n",
    "print '========================'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "opposites = ((['中国', '城市'], ['学生']),\n",
    "             (['男', '工作'], ['女']),\n",
    "             (['俄罗斯', '美国', '英国'], ['日本']))\n",
    "for positive, negative in opposites:\n",
    "    result = model.most_similar(positive=positive, negative=negative)\n",
    "    print_list(positive)\n",
    "    print '-',\n",
    "    print_list(negative)\n",
    "    print '：'\n",
    "    for word, similar in result:\n",
    "        print '\\t', word, similar\n",
    "\n",
    "print '========================'\n",
    "words_list = ('苹果 三星 美的 海尔', '中国 日本 韩国 美国 北京',\n",
    "              '医院 手术 护士 医生 感染 福利', '爸爸 妈妈 舅舅 爷爷 叔叔 阿姨 老婆')\n",
    "for words in words_list:\n",
    "    print words, '离群词：', model.doesnt_match(words.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 2] No such file or directory: '../dataset/200806_segment'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-a86e885969df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLoadCorpora\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../dataset/200806_segment'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mt_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 词向量维度为200，丢弃出现次数少于5次的词\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'200806.model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m'OK:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gensim/models/word2vec.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words)\u001b[0m\n\u001b[1;32m    476\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You can't pass a generator as the sentences argument. Try an iterator.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m             self.train(sentences, total_examples=self.corpus_count, epochs=self.iter,\n\u001b[1;32m    480\u001b[0m                        start_alpha=self.alpha, end_alpha=self.min_alpha)\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gensim/models/word2vec.pyc\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, sentences, keep_raw_vocab, trim_rule, progress_per, update)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \"\"\"\n\u001b[0;32m--> 553\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_per\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress_per\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# initial survey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeep_raw_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep_raw_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# trim by min_count & precalculate downsampling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalize_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# build tables & arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gensim/models/word2vec.pyc\u001b[0m in \u001b[0;36mscan_vocab\u001b[0;34m(self, sentences, progress_per, trim_rule)\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0mchecked_string_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msentence_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchecked_string_types\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-a86e885969df>\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mpath_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;32mprint\u001b[0m \u001b[0mpath_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 2] No such file or directory: '../dataset/200806_segment'"
     ]
    }
   ],
   "source": [
    "# etc/bin/python\n",
    "# -*- encoding: utf-8 -*-\n",
    "\n",
    "from time import time\n",
    "from gensim.models import Word2Vec\n",
    "import sys\n",
    "import os\n",
    "\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "\n",
    "\n",
    "class LoadCorpora(object):\n",
    "    def __init__(self, dir_name):\n",
    "        self.path = dir_name\n",
    "\n",
    "    def __iter__(self):\n",
    "        for file_name in os.listdir(self.path):\n",
    "            path_name = os.path.join(self.path, file_name)\n",
    "            print path_name\n",
    "            f = open(path_name, 'r')\n",
    "            for line in f:\n",
    "                yield line.split(' ')\n",
    "\n",
    "\n",
    "def print_list(a):\n",
    "    for i, s in enumerate(a):\n",
    "        if i != 0:\n",
    "            print '+',\n",
    "        print s,\n",
    "\n",
    "\n",
    "if not os.path.exists('200806.model'):\n",
    "    sentences = LoadCorpora('../dataset/200806_segment')\n",
    "    t_start = time()\n",
    "    model = Word2Vec(sentences, size=200, min_count=5, workers=8)  # 词向量维度为200，丢弃出现次数少于5次的词\n",
    "    model.save('200806.model')\n",
    "    print 'OK:', time() - t_start\n",
    "\n",
    "model = Word2Vec.load('200806.model')\n",
    "print type(model.vocab), len(model.vocab)\n",
    "for i, word in enumerate(model.vocab):\n",
    "    print word,\n",
    "    if i % 50 == 49:\n",
    "        print\n",
    "print\n",
    "\n",
    "intrested_words = ('中国', '手机', '学习')\n",
    "print '特征向量：'\n",
    "for word in intrested_words:\n",
    "    print word, len(model[word]), model[word]\n",
    "for word in intrested_words:\n",
    "    result = model.most_similar(word)\n",
    "    print '与', word, '最相近的词：'\n",
    "    for w, s in result:\n",
    "        print '\\t', w, s\n",
    "\n",
    "words = ('中国', '祖国', '毛泽东', '人民')\n",
    "for i in range(len(words)):\n",
    "    w1 = words[i]\n",
    "    for j in range(i + 1, len(words)):\n",
    "        w2 = words[j]\n",
    "        print '%s 和 %s 的相似度为：%.6f' % (w1, w2, model.similarity(w1, w2))\n",
    "\n",
    "print '========================'\n",
    "opposites = ((['中国', '城市'], ['学生']),\n",
    "             (['男', '工作'], ['女']),\n",
    "             (['俄罗斯', '美国', '英国'], ['日本']))\n",
    "for positive, negative in opposites:\n",
    "    result = model.most_similar(positive=positive, negative=negative)\n",
    "    print_list(positive)\n",
    "    print '-',\n",
    "    print_list(negative)\n",
    "    print '：'\n",
    "    for word, similar in result:\n",
    "        print '\\t', word, similar\n",
    "\n",
    "print '========================'\n",
    "words = '苹果 三星 小米 联想 华为 海尔 格力'\n",
    "print words, '离群词：', model.doesnt_match(words.split(' '))\n",
    "words = '苹果 三星 美的 海尔'\n",
    "print words, '离群词：', model.doesnt_match(words.split(' '))\n",
    "words = '中国 日本 韩国 美国 北京'\n",
    "print words, '离群词：', model.doesnt_match(words.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

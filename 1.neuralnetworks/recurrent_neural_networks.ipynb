{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### RNN\n",
    "* RNN 的结构天然适合时间序列数据的分析和处理，展开的每一层具有相同的结构和参数，和CNN类似可以共享参数\n",
    "* 虽然理论上可以处理较远依赖(long-term dependencies)的数据，但是实际中记忆最深的依然是最后输入的一些信号\n",
    "\n",
    "<img src=\"image/RNN-longtermdependencies.png\" width = \"600\" height = \"400\" alt=\"循环神经网络展开示意图\" align=center />\n",
    "\n",
    "<img src=\"image/LSTM3-SimpleRNN.png\" width = \"600\" height = \"400\" alt=\"循环神经网络展开示意图\" align=center />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### LSTM\n",
    "* RNN 的致命问题在于不能长程依赖，而LSTM 结构天然为解决long-term dependencies问题而设计的,\n",
    "* LSTM 包含point-wise操作如向量点乘、加法等，最主要的在于其Gates 的控制，这些门包含了一些Sigmoid 层和向量点乘操作，Sigmoid 的输出在0-1之间，控制信息传递比例。0代表丢弃(遗忘)信息，1代表记住所有信息。每个LSTM单元包含三个这样的门，用于维护和控制单元状态。\n",
    "* 凭借对于状态信息的存储和修改，LSTM 可以实心长程记忆。\n",
    "\n",
    "<img src=\"image/LSTM3-chain.png\" width = \"600\" height = \"400\" alt=\"循环神经网络展开示意图\" align=center />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### LSTM 结构\n",
    "\n",
    "* 除了一些乘法和加法操作，这个结构可以非常简单的让上一层的输出传送到下一层，并可以控制之下保持信息不变\n",
    "\n",
    "<img src=\"image/LSTM3-C-line.png\" width = \"800\" height = \"400\" alt=\"循环神经网络展开示意图\" align=center />\n",
    "\n",
    "* LSTM 主要依靠门来控制信息该被记住还是遗忘，其包含向量点乘操作，Sigmoid 的输出在0-1之间，控制信息传递比例。0代表丢弃(遗忘)信息，1代表记住所有信息。\n",
    "<img src=\"image/LSTM3-gate.png\" width = \"200\" height = \"100\" alt=\"循环神经网络展开示意图\" align=center />\n",
    "\n",
    "#### 门结构\n",
    "1、 forget gate layer\n",
    " * LSTM 第一步主要是依赖“遗忘门”控制那些信息该被遗忘丢弃, 主要依赖Sigmoid 函数来控制. 以此来控制上一个状态 $C_{t-1}$是否需要继续传输\n",
    "<img src=\"image/LSTM3-focus-f.png\" width = \"800\" height = \"400\" alt=\"循环神经网络展开示意图\" align=center />\n",
    "\n",
    "2、input gate layer\n",
    "\n",
    " * LSTM 第二步主要是依赖“输入存储”控制那些信息该被存储更新, 其中控制门输出 $i_t$ 则是用来控制是否需要更新，而$tanh$ 分支则计算出一个候选新值 $\\hat C_t$\n",
    " \n",
    " <img src=\"image/LSTM3-focus-i.png\" width = \"800\" height = \"400\" alt=\"循环神经网络展开示意图\" align=center />\n",
    " \n",
    " * $\\hat C_t$ 则可以被加在原来的分之，用于替换即将被遗忘的旧值，更新状态\n",
    " * 此时旧的状态$C_{t-1}$可以被替换为新的值$C_t$, $f_t$ 用于控制遗忘程度，而 $i_t$则用于控制更新程度\n",
    " <img src=\"image/LSTM3-focus-C.png\" width = \"800\" height = \"400\" alt=\"循环神经网络展开示意图\" align=center />\n",
    " \n",
    "3、output gate layer\n",
    "\n",
    " * 输出 $o_t$ 则是用来控制輸出那些數據，而$tanh(C_t)$ 分支则计算出最終輸出值 $h_t$\n",
    " \n",
    " <img src=\"image/LSTM3-focus-o.png\" width = \"800\" height = \"400\" alt=\"循环神经网络展开示意图\" align=center />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

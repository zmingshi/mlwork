{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性回归\n",
    "$$h_{\\vec w} (\\mathbf x) = \\sum_{i=0}^nw_i x_i = w^T \\mathbf x$$\n",
    "\n",
    "### 1. 似然函数\n",
    "$$y^{(i)} = w^Tx^{(i)} + \\epsilon^{(i)}$$\n",
    "* 其中$x^{(i)}$ 为第$i$个样本, 误差$\\epsilon^{(i)}$ 服从均值为0，方差为$\\sigma^2$ 的高斯分布(中心极限定理), 共$m$ 个样本.\n",
    "$$p\\left(\\epsilon^{(i)}\\right) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(- \\frac{\\left(\\epsilon^{(i)}\\right)^2}{2\\sigma^2} \\right)$$\n",
    "\n",
    "$$p\\left(y^{(i)}|x^{(i)};w \\right) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(- \\frac{\\left(y^{(i)} - w^Tx^{(i)}\\right)^2}{2\\sigma^2} \\right)$$\n",
    "\n",
    "$$\\begin{align}\n",
    "L(w) &= \\prod_{i=1}^m p\\left(y^{(i)}|x^{(i)};w \\right) \\\\\n",
    "          &= \\prod_{i=1}^m \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(- \\frac{\\left(y^{(i)} - w^Tx^{(i)}\\right)^2}{2\\sigma^2} \\right)\n",
    "\\end{align}$$\n",
    "\n",
    "* 上述似然函数取对数得到如下结果：\n",
    "$$\\begin{align}\n",
    "l(w) &= \\log L(w) \\\\\n",
    "          &= \\log \\prod_{i=1}^m \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(- \\frac{\\left(y^{(i)} - w^Tx^{(i)}\\right)^2}{2\\sigma^2} \\right) \\\\\n",
    "          &= \\sum_{i=1}^m\\log \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(- \\frac{\\left(y^{(i)} - w^Tx^{(i)}\\right)^2}{2\\sigma^2} \\right) \\\\\n",
    "          &= m\\log \\frac{1}{\\sqrt{2\\pi\\sigma^2}} - \\frac{1}{2\\sigma^2}\\sum_{i=1}^m\\left(y^{(i)} - w^Tx^{(i)}\\right)^2\n",
    "\\end{align}$$\n",
    "* 最小二乘损失函数如下:\n",
    "$$\\begin{align}\n",
    "J(w) &= \\frac{1}{2}\\sum_{i=1}^m\\left(w^Tx^{(i)} - y^{(i)}\\right)^2 \\\\\n",
    "&= \\frac{1}{2}(Xw - y)^T(Xw-y)\n",
    "\\end{align}$$\n",
    "\n",
    "* 求解梯度得到驻点，从而得到解析结果：\n",
    "$$w = \\left(X^TX\\right)^{-1}X^Ty$$\n",
    "* 防止$X^TX$ 不可逆或者说过拟合, 加入$\\lambda$ 扰动：\n",
    "$$w = \\left(X^TX + \\lambda I\\right)^{-1}X^Ty$$\n",
    "* 由于$X^TX$为半正定矩阵，则对于任意$\\lambda > 0$， 则$\\left(X^TX + \\lambda I\\right)$ 一定正定，从而保证可逆： 对于任意非零向量$u$,  $u^T\\left(X^TX + \\lambda I\\right)u = u^TX^TXu + \\lambda u^Tu$ > 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 正则化\n",
    "$\\forall \\lambda>0, \\rho \\in [0,1]$, 正则化定义如下：\n",
    "* L2-Norm : $ J(\\vec w) = \\frac{1}{2}\\sum_{i=1}^m\\left(h_{\\vec w}(x^{(i)}) - y^{(i)}\\right)^2 + \\lambda \\sum_{i=1}^nw_i^2  $\n",
    "* L1-Norm : $ J(\\vec w) = \\frac{1}{2}\\sum_{i=1}^m\\left(h_{\\vec w}(x^{(i)}) - y^{(i)}\\right)^2 + \\lambda \\sum_{i=1}^n|w_i|  $\n",
    "* Elastic-Net : $ J(\\vec w) = \\frac{1}{2}\\sum_{i=1}^m\\left(h_{\\vec w}(x^{(i)}) - y^{(i)}\\right)^2 + \\lambda \\left(\\rho\\cdot \\sum_{i=1}^n|w_i| + (1-\\rho) \\cdot \\sum_{i=1}^nw_i^2\\right)  $\n",
    "* 正则化可以用来防止过拟合，其中L1 正则可以产生稀疏解，L2 正则可以产生平滑解，从贝叶斯的角度来讲，正则化等价于对模型参数引入先验分布，L2 正则假定参数先验服从高斯分布，L1 正则假定参数先验服从拉普拉斯分布。参考：http://charleshm.github.io/2016/03/Regularized-Regression/\n",
    "\n",
    "#### 2.1. Ridge Regression\n",
    "对参数 $w$ 引入协方差为$\\alpha^2$的零均值高斯先验, 用最大后验估计(MAP)\n",
    "\n",
    "$$\\begin{align}\n",
    "L(w) &= p(y|w;x)p(w) \\\\\n",
    " &= \\prod_{i=1}^m \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{\\left(y^{(i)} - w^Tx^{(i)}\\right)^2}{2\\sigma^2} \\right) \n",
    "  \\prod_{i=1}^j\\frac{1}{\\sqrt{2\\pi\\alpha^2}} \\exp\\left(- \\frac{\\left(w_j\\right)^2}{2\\alpha^2} \\right) \\\\\n",
    " &= \\prod_{i=1}^m \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(- \\frac{\\left(y^{(i)} - w^Tx^{(i)}\\right)^2}{2\\sigma^2} \\right) \n",
    "  \\frac{1}{\\sqrt{2\\pi\\alpha^2}} \\exp\\left(- \\frac{\\left(w^Tw\\right)}{2\\alpha^2} \\right)\n",
    "\\end{align}$$\n",
    "\n",
    "取对数：\n",
    "$$\\begin{align}\n",
    "l(w) &= \\log L(w) \\\\\n",
    "          &= m\\log \\frac{1}{\\sqrt{2\\pi\\sigma^2}} + n \\log \\frac{1}{\\sqrt{2\\pi\\alpha^2}} - \\frac{1}{2\\sigma^2}\\sum_{i=1}^m\\left(y^{(i)} - w^Tx^{(i)}\\right)^2 - \\frac{1}{2\\alpha^2} w^Tw \\\\\n",
    "          & \\implies w_{MAP} = arg min_w \\left(\\frac{1}{2\\sigma^2}\\sum_{i=1}^m\\left(y^{(i)} - w^Tx^{(i)}\\right)^2 + \\frac{1}{2\\alpha^2} w^Tw \\right)\n",
    "\\end{align}$$\n",
    "等价于损失函数形式如下：\n",
    "$$ J_R(w) = \\frac{1}{m}\\left(y -w^TX\\right)^2 + \\lambda ||w||_2  $$\n",
    "ridge regression 并不具有产生稀疏解的能力，也就是说参数并不会真出现很多零。假设我们的预测结果与两个特征相关，L2正则倾向于综合两者的影响，给影响大的特征赋予高的权重；而L1正则倾向于选择影响较大的参数，而舍弃掉影响较小的那个。实际应用中 L2 正则表现往往会优于 L1正则，但 L1 正则会大大降低计算量。\n",
    "\n",
    "#### 2.2. Lasso Regression\n",
    "拉普拉斯分布如下：\n",
    "$$ f(x|u,b) =  \\frac{1}{2b} \\exp\\left( -\\frac{x-u}{b}\\right)$$\n",
    "对参数 $w$ 引入拉普拉斯先验分布($u=0$)，则容易得到：\n",
    "$$w_{MAP} = arg min_w \\left(\\frac{1}{2\\sigma^2}\\sum_{i=1}^m\\left(y^{(i)} - w^Tx^{(i)}\\right)^2 + \\frac{1}{2b^2} ||w||_1 \\right)$$\n",
    "#### 2.3. Elastic-Net\n",
    "同样假设$w$ 服从如下分布：\n",
    "$$w \\sim C(\\lambda,\\alpha)\\exp(\\lambda |w|_2 + \\alpha|w|_1)$$\n",
    "可以得到如下结果：\n",
    "$$w_{MAP} = arg min_w |y - Xw|_2 + \\lambda_1 ||w||_2 + \\lambda_2||w||_2$$\n",
    "\n",
    "正则化参数等价于对参数引入 先验分布，使得 模型复杂度 变小（缩小解空间），对于噪声以及outliers的鲁棒性增强（泛化能力）。整个最优化问题从贝叶斯观点来看是一种贝叶斯最大后验估计，其中 正则化项 对应后验估计中的 先验信息 ，损失函数对应后验估计中的似然函数，两者的乘积即对应贝叶斯最大后验估计的形式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Logistic Regression\n",
    "$$h_{\\vec w} (\\mathbf x) = g(w^T \\mathbf x) =  \\frac{1}{1+\\exp(-w^T \\mathbf x)}$$\n",
    "$$g'(x) = \\left(\\frac{1}{1+\\exp(x)}\\right)' = g(x)(1 - g(x)) $$\n",
    "设定：\n",
    "$$p\\left(y=1|x;w\\right) = h_{\\vec w} (\\mathbf x) = g(w^T \\mathbf x) =  \\frac{1}{1+\\exp(-w^T \\mathbf x)}$$\n",
    "$$p\\left(y=0|x;w\\right) = 1 - h_{\\vec w} (\\mathbf x) = g(-w^T \\mathbf x) =  \\frac{1}{1+\\exp(w^T \\mathbf x)}$$\n",
    "通用表示如下：\n",
    "$$p\\left(y|x;w\\right) = (h_{\\vec w}(\\mathbf x))^y (1-h_{\\vec w}(\\mathbf x))^{(1-y)}$$\n",
    "\n",
    "### 似然函数\n",
    "$$\\begin{align}\n",
    "L(w) &= p(y|X;w) \\\\\n",
    "     &= \\prod_{i=1}^m p\\left(y^{(i)}|x^{(i)};w \\right) \\\\\n",
    "     &= \\prod_{i=1}^m (h_{\\vec w}(\\mathbf x^{(i)}))^{y^{(i)}} (1-h_{\\vec w}(\\mathbf x^{(i)}))^{(1-{y^{(i)}})}\n",
    "\\end{align}$$\n",
    "\n",
    "* 上述似然函数取对数得到如下结果：\n",
    "$$\\begin{align}\n",
    "l(w) &= \\log L(w) \\\\\n",
    "          &= \\sum_{i=1}^m {y^{(i)}}\\log(h_{\\vec w}(\\mathbf x^{(i)})) +  (1 -y^{(i)})\\log(1-h_{\\vec w}(\\mathbf x^{(i)}))\n",
    "\\end{align}$$\n",
    "* 求导如下:\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial l(w)}{\\partial w_i} &=  \\sum_{i=1}^m \\left(\\frac{y^{(i)}}{h_{\\vec w}(\\mathbf x^{(i)})} - \\frac{1-y^{(i)}}{1 - h_{\\vec w}(\\mathbf x^{(i)})} \\right)\\frac{\\partial h(wx)}{\\partial w_i}  \\\\\n",
    "          &= \\sum_{i=1}^m \\left(y^{(i)} - g\\left(\\vec w x^{(i)}\\right)\\right)x_j^{(i)}\n",
    "\\end{align}$$\n",
    "\n",
    "* 参数更新:\n",
    "$$w_i := w_i + \\alpha \\sum_{i=1}^m \\left(y^{(i)} - g\\left(\\vec w x^{(i)}\\right)\\right)x_j^{(i)}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "### 损失函数\n",
    "1. 假设：$y_i \\in \\{0, 1\\}$, $\\hat y_i = \\begin{cases} p_i, & y_i = 1 \\\\ 1-p_i, & y_i = 0 \\end{cases}$\n",
    "$$\\begin{align}\n",
    "L(w) &= \\prod_{i=1}^m p_i^{y^i} (1-p_i)^{1-y^i} \\\\\n",
    "    &\\implies  l(w) = \\sum_{i=1}^m \\ln\\left[p_i^{y^i} (1-p_i)^{1-y^i}\\right] \\\\\n",
    "    & \\underrightarrow{p_i = \\frac{1}{1+exp(-f_i)}} l(w) = \\sum_{i=1}^m \\ln\\left[ \\left({\\frac{1}{1+e^{-f_i}}}\\right)^{y^i} \\left(\\frac{1}{1+e^{f_i}}\\right)^{1-y^i} \\right] \\\\\n",
    "   & \\therefore loss(y_i, \\hat y_i) = -l(w) \\\\\n",
    "   &= \\sum_{i=1}^m \\left[ y_i \\ln \\left(1+e^{-f_i} \\right) + (1-y_i)\\ln \\left(1+e^{f_i} \\right) \\right]\n",
    "\\end{align}$$\n",
    "\n",
    "2. 假设：$y_i \\in \\{1, -1\\}$, $\\hat y_i = \\begin{cases} p_i, & y_i = 1 \\\\ 1-p_i, & y_i = -1 \\end{cases}$\n",
    "$$\\begin{align}\n",
    "L(w) &= \\prod_{i=1}^m p_i^{(y^i+1)/2} (1-p_i)^{-(y^i-1)/2} \\\\\n",
    "    &\\implies  l(w) = \\sum_{i=1}^m \\ln\\left[p_i^{(y^i+1)/2} (1-p_i)^{-(y^i-1)/2}\\right] \\\\\n",
    "    & \\underrightarrow{p_i = \\frac{1}{1+exp(-f_i)}} l(w) = \\sum_{i=1}^m \\ln\\left[ \\left({\\frac{1}{1+e^{-f_i}}}\\right)^{(y^i+1)/2} \\left(\\frac{1}{1+e^{f_i}}\\right)^{-(y^i-1)/2} \\right] \\\\\n",
    "   & \\therefore loss(y_i, \\hat y_i) = -l(w) \\\\\n",
    "   &= \\sum_{i=1}^m \\left[(y^i+1)/2 \\ln \\left(1+e^{-f_i} \\right) -(y^i-1)/2 \\ln \\left(1+e^{f_i} \\right) \\right] \\\\\n",
    "   &= \\begin{cases} \\sum_{i=1}^m \\ln\\left[\\left(1+e^{-f_i}\\right)\\right], & y_i = 1 \\\\ \\sum_{i=1}^m \\ln\\left[\\left(1+e^{f_i}\\right)\\right], & y_i = -1 \\end{cases} \\\\\n",
    "   &= \\sum_{i=1}^m \\ln\\left[\\left(1+e^{-y_i\\cdot f_i}\\right)\\right]\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## LIBFM\n",
    "$$\\hat y(\\mathbf x) = w_0 + \\sum_{i=1}^n w_ix_i + \\sum_{i=1}^n\\sum_{j=i+1}^n \\langle\\mathbf v_i, \\mathbf v_j\\rangle x_ix_j$$\n",
    "* $\\mathbf w \\in \\mathbb R^n, \\mathbf V \\in \\mathbb R^{n*k}, \\langle\\mathbf v_i, \\mathbf v_j\\rangle = \\sum_{f=1}^k v_{i,f} \\cdot v_{j,f}$\n",
    "* 其中$ \\sum_{i=1}^n\\sum_{j=i+1}^n \\langle\\mathbf v_i, \\mathbf v_j\\rangle x_ix_j$ 可以做如下化简：\n",
    "$$\\begin{align}  \n",
    "&\\sum_{i=1}^n\\sum_{j=i+1}^n \\langle\\mathbf v_i, \\mathbf v_j\\rangle x_ix_j \\\\\n",
    "=& \\frac{1}{2}\\left( \\sum_{i=1}^n\\sum_{j=1}^n \\langle\\mathbf v_i, \\mathbf v_j\\rangle x_ix_j  - \\sum_{i=1}^n \\langle\\mathbf v_i, \\mathbf v_i\\rangle x_ix_i \\right) \\\\\n",
    "=& \\frac{1}{2}\\left( \\sum_{i=1}^n\\sum_{j=1}^n\\sum_{f=1}^k v_{i,f}v_{j,f}x_ix_j  - \\sum_{i=1}^n \\sum_{f=1}^k v_{i,f}v_{i,f} x_ix_i \\right) \\\\\n",
    "=& \\frac{1}{2}\\sum_{f=1}^k \\left( \\sum_{i=1}^nv_{i,f}x_i\\sum_{j=1}^nv_{j,f}x_j  - \\sum_{i=1}^n v_{i,f}^2 x_i^2 \\right) \\\\\n",
    "=& \\frac{1}{2}\\sum_{f=1}^k \\left( \\left(\\sum_{i=1}^nv_{i,f}x_i \\right)^2  - \\sum_{i=1}^n v_{i,f}^2 x_i^2 \\right)\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
